{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Project: **Advanced Finding Lane Lines on the Road** \n",
    "\n",
    "***\n",
    "In this project, advanced computer vision technologies are applied to find lane lines and identify the drivable lane area ahead.\n",
    "\n",
    "The project first calibrates the camera based on the calibration pattern images, then it goes through following steps for finding lane lines:\n",
    "1. Correct distortion for image/video frame\n",
    "2. Derive binary image based on the thresholds combination of color space and gradients\n",
    "3. Convert to birds-eye's view through perspective transform\n",
    "4. Detect lane lines\n",
    "   a. Use histogram and sliding window to find pixels belongs to lane lines\n",
    "   b. Or search lane lines based on the line positions in previous video frame \n",
    "5. Draw lane line and drivable lane area\n",
    "6. Calculate and check lane information (curvature, vehicle position relative to lane center, lane width)\n",
    "7. Convert back to original perspective\n",
    "8. Visualize the image/video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Calibration\n",
    "In this section, the parameters for calibrating the camera used in this project are calculated based on a set of calibration images. The calibration parameters include: camera matrix, distortion coefficients, camera position vectors (rotation and translation).\n",
    "\n",
    "A class (Cam) is defined for storing the calibration parameters for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cam:\n",
    "    '''\n",
    "    Class for storing the calibration parameters of a camera.\n",
    "    It stores: camera matrix (mtx), distortion coefficients (dist)，\n",
    "               rotation vector (rvecs), translation vector (tvecs).\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.mtx = None\n",
    "        self.dist = None\n",
    "        self.rvecs = None\n",
    "        self.tvecs = None\n",
    "    \n",
    "    def set_calibration_param(self, mtx, dist, rvecs, tvecs):\n",
    "        self.mtx = mtx\n",
    "        self.dist = dist\n",
    "        self.rvecs = rvecs\n",
    "        self.tvecs = tvecs\n",
    "\n",
    "def calibrate_camera(images, cols, rows):\n",
    "    '''\n",
    "    This function calculates the calibration parameters for a camera.\n",
    "    images:  a set of chessboard pattern images, which are taken in different scenarios.\n",
    "    cols:  Number of inside corners in each row in chessboard pattern images\n",
    "    rows:  Number of inside corners in each column in chessboard pattern images\n",
    "    '''\n",
    "    objpoints = []   # Points in real world (3D, z = 0)\n",
    "    imgpoints = []   # Points in image plane (2D)\n",
    "\n",
    "    # Prepare object points (same for all images)\n",
    "    objp = np.zeros((rows*cols, 3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:cols,0:rows].T.reshape(-1,2)\n",
    "\n",
    "    gray = None\n",
    "\n",
    "    for image in images:\n",
    "        img = mpimg.imread(image)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (cols, rows), None)\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "    # Calculate calibration and distortion coefficients\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None) \n",
    "    return ret, mtx, dist, rvecs, tvecs\n",
    "\n",
    "\n",
    "print(\"Calibrating Frame...\")\n",
    "images = glob.glob('./camera_cal/calibration*.jpg')\n",
    "ret, mtx, dist, rvecs, tvecs = calibrate_camera(images, 9, 6)\n",
    "\n",
    "# Save the calibration parameters in cam, which could be used later for correcting images and video frames.\n",
    "cam = Cam()\n",
    "cam.set_calibration_param(mtx, dist, rvecs, tvecs)\n",
    "print(\"Calibrating Frame...Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_image(img, cam):\n",
    "    '''\n",
    "    Undistort the image based on the calibration parameter stored in cam\n",
    "    '''\n",
    "    undist_img = cv2.undistort(img, cam.mtx, cam.dist, None, cam.mtx)\n",
    "    return undist_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undistort Image/Video Frames\n",
    "Image and video frames should be undistorted before further steps for finding lanes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test image correction with an example calibration image\n",
    "calibration_image = './camera_cal/calibration1.jpg'\n",
    "cali_img = mpimg.imread(calibration_image)\n",
    "undist_cali_img = correct_image(cali_img, cam)\n",
    "undist_cali_image = './output_images/calibration1_undist.jpg'\n",
    "mpimg.imsave(undist_cali_image, undist_cali_img)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(cali_img)\n",
    "plt.title('Original', fontsize=22)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(undist_cali_img)\n",
    "plt.title('Undistorted', fontsize=22)\n",
    "\n",
    "# Test image correction on test images\n",
    "test_images = glob.glob('./test_images/test*.jpg')\n",
    "tmp_images = glob.glob('./test_images/straight_lines*.jpg')\n",
    "test_images = test_images + tmp_images\n",
    "\n",
    "rows = len(test_images)\n",
    "\n",
    "dst_dir = './output_images/'\n",
    "for image_file in test_images:\n",
    "    (path_filename, ext) = os.path.splitext(image_file)\n",
    "    filename = os.path.basename(path_filename)\n",
    "    undist_image_file = dst_dir + filename + \"_undist\" + ext\n",
    "    img = mpimg.imread(image_file)\n",
    "    undist_img = correct_image(img, cam)\n",
    "    mpimg.imsave(undist_image_file, undist_img)\n",
    "    \n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original', fontsize=22)\n",
    "    ax2.imshow(undist_img)\n",
    "    ax2.set_title('Undistorted', fontsize=22)\n",
    "    \n",
    "print('Undistorting images test done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color and Gradient Thresholding\n",
    "In order to identify the lane lines, the pixels belonging to the lane lines should be detected.\n",
    "To do that, the masks in different color spaces (e.g. RGB, HLS, HSV, Lab, LUV), and also gradient change in different directions and perspectives are defined (e.g. gradient in x-axis, y-axis, magnitude, x-y direction ratio) and combined for extracting the lane pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    '''\n",
    "    Gradient thresholding with Sobel operator along x or y axis\n",
    "    '''\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Take the derivative in x or y given orient = 'x' or 'y'\n",
    "    if orient == 'x':\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    else:\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "\n",
    "    # Take the absolute value of the derivative or gradient\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    #print(abs_sobel.shape)\n",
    "    # Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255 * abs_sobel / np.max(abs_sobel))\n",
    "    # Create a mask of 1's where the scaled gradient magnitude\n",
    "    # is > thresh_min and < thresh_max\n",
    "    sbinary = np.zeros_like(scaled_sobel)\n",
    "    # Return this mask as your binary_output image\n",
    "    sbinary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return sbinary\n",
    "\n",
    "\n",
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    '''\n",
    "    Gradient thresholding with Sobel operator with magnitude\n",
    "    '''\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Take the gradient in x and y separately\n",
    "    sobelX = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobelY = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    # Calculate the magnitude\n",
    "    mag_sobel = np.sqrt(sobelX**2 + sobelY**2)\n",
    "    # Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255 * mag_sobel / np.max(mag_sobel))\n",
    "    # Create a binary mask where mag thresholds are met\n",
    "    sbinary = np.zeros_like(scaled_sobel)\n",
    "    # Return this mask as your binary_output image\n",
    "    sbinary[(scaled_sobel >= mag_thresh[0]) & (scaled_sobel <= mag_thresh[1])] = 1\n",
    "    return sbinary\n",
    "\n",
    "\n",
    "def dir_thresh(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    '''\n",
    "    Gradient thresholding based on direction\n",
    "    '''\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Take the gradient in x and y separately\n",
    "    sobelX = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobelY = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "\n",
    "    # Take the absolute value of the x and y gradients\n",
    "    abs_sobelX = np.absolute(sobelX)\n",
    "    abs_sobelY = np.absolute(sobelY)\n",
    "\n",
    "    # Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient\n",
    "    direction_sobel = np.arctan2(abs_sobelY, abs_sobelX)\n",
    "    # Create a binary mask where direction thresholds are met\n",
    "    sbinary = np.zeros_like(direction_sobel)\n",
    "    # Return this mask as your binary_output image\n",
    "    sbinary[(direction_sobel >= thresh[0]) & (direction_sobel <= thresh[1])] = 1\n",
    "    return sbinary\n",
    "\n",
    "\n",
    "def color_hls_thresh(img, s_thresh=(90, 255)):\n",
    "    # Convert to HLS color space and separate the S channel\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    s_channel = hls[:, :, 2]\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    return s_binary\n",
    "\n",
    "\n",
    "def color_luv_thresh(img, l_thresh=(225, 255)):\n",
    "    # Convert to Luv color space and separate the L channel\n",
    "    luv = cv2.cvtColor(img, cv2.COLOR_RGB2Luv)\n",
    "    l_channel = luv[:,:,0]\n",
    "    l_binary = np.zeros_like(l_channel)\n",
    "    l_binary[(l_channel >= l_thresh[0]) & (l_channel <= l_thresh[1])] = 1\n",
    "    return l_binary\n",
    "\n",
    "\n",
    "def color_lab_thresh(img, b_thresh=(155, 200)):\n",
    "    # Convert to Lab color space and separate the b channel\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n",
    "    b_channel = lab[:,:,2]\n",
    "    b_binary = np.zeros_like(b_channel)\n",
    "    b_binary[(b_channel >= b_thresh[0]) & (b_channel <= b_thresh[1])] = 1\n",
    "    return b_binary\n",
    " \n",
    "    \n",
    "def color_gradient_threshold(img):\n",
    "    '''\n",
    "    This function create binary image by thresholding in color space and/or gradient.\n",
    "    '''\n",
    "    # HLS space\n",
    "    s_binary = color_hls_thresh(img, s_thresh=(170, 255))\n",
    "    \n",
    "    # LUV space\n",
    "    l_binary = color_luv_thresh(img, l_thresh=(225, 255))\n",
    "    \n",
    "    # Lab space\n",
    "    b_binary = color_lab_thresh(img, b_thresh=(155, 200))\n",
    "\n",
    "    # HSV space\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    yellow_min = np.array([20,60,60])\n",
    "    yellow_max = np.array([38,174,250])\n",
    "    yellow = cv2.inRange(hsv, yellow_min, yellow_max)\n",
    "    \n",
    "    # RGB space\n",
    "    white_min = np.array([200,200,200])\n",
    "    white_max = np.array([255,255,255])\n",
    "    white = cv2.inRange(img, white_min, white_max)\n",
    "\n",
    "    # Sobel gradient\n",
    "    ksize = 3\n",
    "    gradx = abs_sobel_thresh(img, orient='x', sobel_kernel=ksize, thresh=(20, 100))\n",
    "    grady = abs_sobel_thresh(img, orient='y', sobel_kernel=ksize, thresh=(20, 100))\n",
    "    mag_binary = mag_thresh(img, sobel_kernel=ksize, mag_thresh=(30, 100))\n",
    "    dir_binary = dir_thresh(img, sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "    \n",
    "    # Stack each channel\n",
    "    color_img = np.dstack((np.zeros_like(gradx), gradx, s_binary)) * 255\n",
    "    #combined_binary = np.zeros_like(gradx)\n",
    "    #combined_binary[(s_binary == 1) | (gradx == 1)] = 1\n",
    "\n",
    "    combined_binary = np.zeros_like(s_binary)\n",
    "    #combined_binary[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1)) | (s_binary == 1)] = 1\n",
    "\n",
    "    combined_binary[(yellow >= 1) | (white >=1) | (l_binary == 1) | (b_binary == 1)] = 1\n",
    "    ##combined_binary[(yellow >= 1) | (white >=1) | (l_binary == 1) | (b_binary == 1) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "\n",
    "    return color_img, combined_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create thresholded binary image based on color transform and gradients\n",
    "for image_file in test_images:\n",
    "    (path_filename, ext) = os.path.splitext(image_file)\n",
    "    filename = os.path.basename(path_filename)\n",
    "    combined_thresholded_image_file = dst_dir + filename + \"_combined_thresholded\" + ext\n",
    "    img = mpimg.imread(image_file)\n",
    "    undist_img = correct_image(img, cam)\n",
    "    color_img, combined_thresholded_img = color_gradient_threshold(undist_img)\n",
    "    mpimg.imsave(combined_thresholded_image_file, combined_thresholded_img*255, cmap = 'gray')\n",
    "    \n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    ax1.imshow(undist_img)\n",
    "    ax1.set_title('Undistorted', fontsize=22)\n",
    "    ax2.imshow(combined_thresholded_img, cmap = 'gray')\n",
    "    ax2.set_title('Color & Binary Thresholded Binary', fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective Transform\n",
    "After creating color and/or gradient thresholded binary image, perspective transform is carried out to convert to a birds-eye view, which allows easily find the curvature of the lanes.\n",
    "\n",
    "In order to perform perspective transform, positions of four points in original image and converted image are defined as follows:\n",
    "\n",
    "Source Points: [594,447], [690,447], [1130,720], [195,720]\n",
    "Target Points: [310,0], [960,0], [960,680], [310,680]\n",
    "\n",
    "Note: After identifying the lane lines intthe birds-eye view image, at the end, the image needs to be converted back (i.e. inverse perspective transform) to original perspective for overlaying the lane lines on the raw image/video frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_transform(img, inverse = False):\n",
    "    '''\n",
    "    Transform the image to birds-eye perspective (inverse=False), \n",
    "    or convert from birds-eye perspective to original perspective\n",
    "    \n",
    "    img: undistorted image to be transformed perspective\n",
    "    '''\n",
    "    # src: 4 source points in original image\n",
    "    src = np.float32([[594,447], [690,447], [1130,720], [195,720]])\n",
    "    # dst: points in target image\n",
    "    dst = np.float32([[310,0], [960,0], [960,680], [310,680]])\n",
    "    # use cv2.getPerspectiveTransform() to get M, the transform matrix\n",
    "    if inverse == False:\n",
    "        M = cv2.getPerspectiveTransform(src, dst)\n",
    "    else:\n",
    "        M = cv2.getPerspectiveTransform(dst, src)\n",
    "    # use cv2.warpPerspective() to warp your image to a top-down view\n",
    "    transformed = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    return transformed, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test perspective transform on test images\n",
    "for image_file in test_images:\n",
    "    (path_filename, ext) = os.path.splitext(image_file)\n",
    "    filename = os.path.basename(path_filename)\n",
    "    transformed_combined_thresholded_image = dst_dir + filename + \"_transformed_combined_thresholded\" + ext\n",
    "    transformed_color_image = dst_dir + filename + \"_transformed_color\" + ext\n",
    "    img = mpimg.imread(image_file)\n",
    "    undist_img = correct_image(img, cam)\n",
    "    color_img, combined_thresholded_img = color_gradient_threshold(undist_img)\n",
    "    \n",
    "    transformed_combined_thresholded_img, M = perspective_transform(combined_thresholded_img)\n",
    "    transformed_color_img, M = perspective_transform(undist_img)\n",
    "    \n",
    "    mpimg.imsave(transformed_combined_thresholded_image, transformed_combined_thresholded_img*255, cmap = 'gray')\n",
    "    mpimg.imsave(transformed_color_image, transformed_color_img)\n",
    "    \n",
    "    _, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 9))\n",
    "    ax1.imshow(undist_img)\n",
    "    ax1.set_title('Original (corrected)')\n",
    "    ax2.imshow(transformed_combined_thresholded_img, cmap = 'gray')\n",
    "    ax2.set_title('Perspective Transformed (Binary)')\n",
    "    ax3.imshow(transformed_color_img)\n",
    "    ax3.set_title('Perspective Transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane Line Pixel Detection\n",
    "To find the pixels belonging to the lane lines, a histogram is calculated along the y axis of the warped binary image.\n",
    "The histogram shows two peaks, which corresponds the base positions of the two lane lines at the bottom of the image.\n",
    "From these base positions, sliding windows are defined and used to search for the pixels for the lane lines towards the top of the images.\n",
    "The searching process keeps updating the base positions for the sliding windows based on the distribution of the lane line pixels in the previous window.\n",
    "The function find_lane_pixels() carries out the lane pixel searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lane_pixels(binary_warped):\n",
    "    '''\n",
    "    Function for searching pixels belonging to lane lines.\n",
    "    '''\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)\n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # HYPERPARAMETERS\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "\n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    window_height = np.int(binary_warped.shape[0]//nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        # Find the four below boundaries of the window ###\n",
    "        win_xleft_low = max(0, leftx_current - margin)\n",
    "        win_xleft_high = min(binary_warped.shape[1], leftx_current + margin)\n",
    "        win_xright_low = max(0, rightx_current - margin)\n",
    "        win_xright_high = min(binary_warped.shape[1], rightx_current + margin)\n",
    "        \n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),\n",
    "                              (win_xleft_high,win_y_high),(255,255,0), 3) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),\n",
    "                              (win_xright_high,win_y_high),(255,255,0), 3) \n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        leftWin = (nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)\n",
    "        good_left_inds = leftWin.nonzero()[0]\n",
    "        rightWin = (nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)\n",
    "        good_right_inds = rightWin.nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        # If found > minpix pixels, recenter next window\n",
    "        # (`right` or `leftx_current`) on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "\n",
    "        if len(good_right_inds) > minpix:\n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "    try:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    except ValueError:\n",
    "        # Avoids an error if the above is not implemented fully\n",
    "        pass\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    \n",
    "    # Colors in the left and right lane regions\n",
    "    out_img[lefty, leftx] = [0, 255, 0]\n",
    "    out_img[righty, rightx] = [0, 0, 255]\n",
    "\n",
    "    return leftx, lefty, rightx, righty, histogram, out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for lane pixel searching\n",
    "test_image = './test_images/straight_lines1.jpg'\n",
    "img = mpimg.imread(test_image)\n",
    "undist_img = correct_image(img, cam)\n",
    "color_img, combined_thresholded_img = color_gradient_threshold(undist_img)\n",
    "transformed_combined_thresholded_img, M = perspective_transform(combined_thresholded_img)\n",
    "leftx, lefty, rightx, righty, histogram, sliding_win_img = find_lane_pixels(transformed_combined_thresholded_img)\n",
    "\n",
    "sliding_win_image = './output_images/straight_lines1_slidingwin.jpg'\n",
    "mpimg.imsave(sliding_win_image, sliding_win_img)\n",
    "\n",
    "histogram_image = './output_images/straight_lines1_histogram.jpg'\n",
    "\n",
    "plt.plot(histogram)\n",
    "plt.title(\"Histogram of Pixels\", fontsize=22)\n",
    "plt.savefig(histogram_image)\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "ax1.imshow(transformed_combined_thresholded_img, cmap = 'gray')\n",
    "ax1.set_title('Perspective Transformed (Binary)', fontsize=22)\n",
    "ax2.imshow(sliding_win_img)\n",
    "ax2.set_title('Sliding Windows', fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Polynomial Lines\n",
    "Once find the pixels belonging to the lane lines, we can fit polynomial lines to the pixels for drawing the lane lines later.\n",
    "\n",
    "The following function search_lane_raw() fits two polynomial lines (i.e. the left and right lane lies) to the pixels detected in previous steps. It returns the coefficients of the polynomial lines as expression f(y) = A*y^2 + B*y + C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sliding_window(binary_warped):\n",
    "    # Find our lane pixels first\n",
    "    leftx, lefty, rightx, righty, histogram, out_img = find_lane_pixels(binary_warped)\n",
    "    if (len(lefty) < 20) or (len(righty) < 20):\n",
    "        return False, None, None, None\n",
    "    \n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    return True, left_fit, right_fit, out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the coefficients of the polynomial lines, we could use following function cal_left_right_fitx() to derive positions of the pixels forming the lane lines.\n",
    "\n",
    "The function draw_lane() calls cal_left_right_fitx() to obtains pixels of fitted lane lines, and then draw the lane lines and drivable lane area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_left_right_fitx(img, left_fit, right_fit):\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, img.shape[0]-1, img.shape[0] )\n",
    "    try:\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    except TypeError:\n",
    "        # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "        print('The function failed to fit a line!')\n",
    "        left_fitx = 1*ploty**2 + 1*ploty\n",
    "        right_fitx = 1*ploty**2 + 1*ploty\n",
    "        \n",
    "    return left_fitx, right_fitx, ploty\n",
    "\n",
    "\n",
    "def draw_lane(binary_warped, left_fit, right_fit):\n",
    "    # Draw lane lines and drivable lane area\n",
    "    left_fitx, right_fitx, ploty = cal_left_right_fitx(binary_warped, left_fit, right_fit)\n",
    "\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))\n",
    "    window_img = np.zeros_like(out_img)\n",
    "    left_line_win = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    right_line_win = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    lane_area_pts = np.hstack((left_line_win, right_line_win))\n",
    "    cv2.polylines(window_img, np.int_([lane_area_pts]), isClosed=False, color=(255,0,0), thickness=20)\n",
    "    cv2.fillPoly(window_img, np.int_([lane_area_pts]), (0,255,0))\n",
    "    result = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "\n",
    "    return window_img, left_fitx, right_fitx, ploty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test drawing lane lines and drivable lane area\n",
    "test_image = './test_images/straight_lines1.jpg'\n",
    "img = mpimg.imread(test_image)\n",
    "undist_img = correct_image(img, cam)\n",
    "color_img, combined_thresholded_img = color_gradient_threshold(undist_img)\n",
    "transformed_combined_thresholded_img, M = perspective_transform(combined_thresholded_img)\n",
    "found, left_fit, right_fit, sliding_win_img = search_sliding_window(transformed_combined_thresholded_img)\n",
    "lane_img, left_fitx, right_fitx, ploty = draw_lane(transformed_combined_thresholded_img, left_fit, right_fit)\n",
    "\n",
    "left_line_win = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "right_line_win = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "lane_area_pts = np.hstack((left_line_win, right_line_win))\n",
    "cv2.polylines(sliding_win_img, np.int_([lane_area_pts]), isClosed=False, color=(255,0,0), thickness=5)\n",
    "\n",
    "lane_area_image = './output_images/straight_lines1_drivable_area.jpg'\n",
    "mpimg.imsave(lane_area_image, lane_img)\n",
    "\n",
    "line_fitted_image = './output_images/straight_lines1_fitted_line.jpg'\n",
    "mpimg.imsave(line_fitted_image, sliding_win_img)\n",
    "\n",
    "_, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 9))\n",
    "ax1.imshow(transformed_combined_thresholded_img, cmap = 'gray')\n",
    "ax1.set_title('Perspective Transformed (Binary)', fontsize=22)\n",
    "ax2.imshow(sliding_win_img)\n",
    "ax2.set_title('Lane Lines Fitted', fontsize=22)\n",
    "ax3.imshow(lane_img)\n",
    "ax3.set_title('Lane Lines and Drivable Lane Area', fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane Curvature and Vehicle Position\n",
    "After identifying the lane lines and drivable area, we can calculate the radius of the curvature of the lane.\n",
    "\n",
    "    Radius = (1+(2*A*y + B)^2)^(3/2)/abs(2*A)\n",
    "\n",
    "In the formula, A, B are the coefficients of the polynomial line corresponding to the lane line. y is the base position of the lane line at the bottom of the image. \n",
    "\n",
    "Based on the base positions of the two lane lines, we can estimate the offset of the vehicle from the lane center.\n",
    "The center of the lane is the center of the two base positions of the lane lines at the bottom edge. The vehicle's center position is at the center of the image's x axis. The difference of these lane center and the vehicle center is the offset of the vehicle from the lane center.\n",
    "\n",
    "So far, all the calculation is based on pixel. To make it practical, we need to calculate the lane curvature and vehicle position in practical units.\n",
    "The relationship between pixel and real length is defined as follows:\n",
    "\n",
    " - 720 pixels in y axis equal to 30 meters in reality\n",
    " - 700 pixels in x axis equal to 3.7 meters (which is the width of a lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_curvature_real(leftx, rightx, ploty):\n",
    "    '''\n",
    "    Calculates the curvature of polynomial functions in meters.\n",
    "    '''\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    \n",
    "    leftx = leftx[::-1]  # Reverse to match top-to-bottom in y\n",
    "    rightx = rightx[::-1]  # Reverse to match top-to-bottom in y\n",
    "\n",
    "    # Fit a second order polynomial to pixel positions in lane line\n",
    "    left_fit_cr = np.polyfit(ploty * ym_per_pix, leftx * xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(ploty * ym_per_pix, rightx * xm_per_pix, 2)\n",
    "    \n",
    "    # Define y-value where we want radius of curvature\n",
    "    # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty) * ym_per_pix\n",
    "    \n",
    "    # calculation of R_curve (radius of curvature)\n",
    "    left_curverad = (1 + (2*left_fit_cr[0]*y_eval + left_fit_cr[1])**2)**1.5/np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = (1 + (2*right_fit_cr[0]*y_eval + right_fit_cr[1])**2)**1.5/np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    return left_curverad, right_curverad\n",
    "\n",
    "\n",
    "def cal_car_pos(img, left_fit, right_fit):\n",
    "    '''\n",
    "    Calculate the offset of the vehicle from the lane center,\n",
    "    and also the detected lane width\n",
    "    '''\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    leftx = left_fit[0]*img.shape[0]**2 + left_fit[1]*img.shape[0] + left_fit[2]\n",
    "    rightx = right_fit[0]*img.shape[0]**2 + right_fit[1]*img.shape[0] + right_fit[2]\n",
    "    car_pos_offset = (img.shape[1]/2 - (rightx + leftx)/2) * xm_per_pix\n",
    "    lane_width = (rightx - leftx)*xm_per_pix\n",
    "    \n",
    "    return car_pos_offset, lane_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check\n",
    "For each lane line detection, we need to check whether the detection is valid.\n",
    "In this project, we go through sanity check for the calculated radius of lane curvature, vehicle offset from lane center and lane width.\n",
    "If the detected lane lines are valid, they should have similar curvature; the vehicle should be in certain offset from the lane center and the lane width should be a valid value.\n",
    "\n",
    "The following function does the sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_lane(left_curverad, right_curverad, car_offset, lane_width):\n",
    "    '''\n",
    "    Sanity check for the detected lane curvature, vehicle offset and lane width.\n",
    "    '''\n",
    "    ratio = left_curverad / right_curverad\n",
    "    if (ratio > 5) or (ratio < 0.2):\n",
    "        return False\n",
    "\n",
    "    if abs(car_offset) > 1.0:\n",
    "        return False\n",
    "    \n",
    "    if (lane_width < 2) or (lane_width > 4.4):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Back to Original Perspective\n",
    "Once we have a valid detection of the lane lines and have drawn the drivable area (as shown in previous sections), we need to convert the image back to the original perspective for visualization.\n",
    "This is done through inverse perspective transform as illustrated below. \n",
    "\n",
    "The code below also shows the final image/video frame with the identified lane lines and drivable area overlaid on the original image/video frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_img(img, initial_img, α=0.8, β=1., γ=0.):\n",
    "    \"\"\" img is the output of the hough_lines(), An image with lines drawn on it. \n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    `initial_img` should be the image before any processing.\n",
    "\n",
    "    The result image is computed as follows:\n",
    "    initial_img * α + img * β + γ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, γ)\n",
    "\n",
    "\n",
    "# Convert back to original perspective\n",
    "test_image = './test_images/straight_lines1.jpg'\n",
    "img = mpimg.imread(test_image)\n",
    "undist_img = correct_image(img, cam)\n",
    "color_img, combined_thresholded_img = color_gradient_threshold(undist_img)\n",
    "transformed_combined_thresholded_img, M = perspective_transform(combined_thresholded_img)\n",
    "found, left_fit, right_fit, sliding_win_img = search_sliding_window(transformed_combined_thresholded_img)\n",
    "lane_img, left_fitx, right_fitx, ploty = draw_lane(transformed_combined_thresholded_img, left_fit, right_fit)\n",
    "\n",
    "inv_transformed_lane_img, M = perspective_transform(lane_img, inverse=True)\n",
    "result_img = weighted_img(inv_transformed_lane_img, img, 1, 0.4, 0)\n",
    "\n",
    "inv_lane_area_image = './output_images/straight_lines1_inv_transformed.jpg'\n",
    "mpimg.imsave(inv_lane_area_image, inv_transformed_lane_img)\n",
    "\n",
    "overlaid_lane_area_image = './output_images/straight_lines1_lane_overlaid.jpg'\n",
    "mpimg.imsave(overlaid_lane_area_image, result_img)\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "ax1.imshow(inv_transformed_lane_img)\n",
    "ax1.set_title('Original Perspective', fontsize=22)\n",
    "ax2.imshow(result_img)\n",
    "ax2.set_title('Lane Line and Drivable Area Overlaid', fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Lane Finding in Video\n",
    "When we search lane lines in a series of video frames, the positions of the lane lines from one frame to the next will not change significantly. (Camera usually streams frames at a rate 24+ fps.)\n",
    "Once we find the lane lines in a frame, in the next frame, we don't need to search blindly with sliding windows as implemented in search_sliding_window(). Instead, we can search the line in the new frame based on the information of the previously fitted polynomial lines in previous frame, which is done in function search_around_poly() defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_around_poly(binary_warped, left_fit, right_fit, margin = 100):\n",
    "    '''\n",
    "    Search polynomial lines in the frame based on line position in previous frame(s).\n",
    "    left_fit, right_fit: The polynomial coefficients of the lane lines in previous frame(s).\n",
    "    '''\n",
    "    # Grab activated pixels\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Set the area of search based on activated x-values\n",
    "    # within the +/- margin of our polynomial function\n",
    "    left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] - margin)) &\n",
    "                      (nonzerox < (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin)))\n",
    "    right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] - margin)) &\n",
    "                       (nonzerox < (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] + margin)))\n",
    "    \n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    if (len(lefty) < 20) or (len(righty) < 20):\n",
    "        return False, None, None\n",
    "\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    return True, left_fit, right_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To smooth the lane line detection and drawing, a LaneLine class is defined, which is used to keep tracking the lane line information in recent frames. \n",
    "The parameters of lane lines to be drawn in current frame are averaged over the parameters in recent frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneLine():\n",
    "    '''\n",
    "    This class keeps tracking the lane lines identified in the recent frames.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.last_detected = False  \n",
    "        # Coefficients of left/right polynomial lines in recent frames\n",
    "        self.recent_fit_left = []\n",
    "        self.recent_fit_right = []\n",
    "        # Current coefficient of left/right polynomial line to be used (for searching in next frame)\n",
    "        self.best_fit_left = None\n",
    "        self.best_fit_right = None\n",
    "        # Number of continuous frames failed to be found lane lines\n",
    "        # If continuously failed certain times, switch back to raw sliding window search\n",
    "        self.cont_bad_frames = 0\n",
    "        # Total frame count\n",
    "        self.frame_cnt = 0\n",
    "        # Threshold for recent_fit_left\n",
    "        self.iterative_win = 3\n",
    "        # Threshold for continuous bad frames\n",
    "        self.bad_frames_threshold = 3\n",
    "        \n",
    "    def update(self, left_fit, right_fit):\n",
    "        self.frame_cnt = self.frame_cnt + 1\n",
    "        if (left_fit is None) or (right_fit is None):\n",
    "            self.last_detected = False\n",
    "            self.cont_bad_frames = self.cont_bad_frames + 1\n",
    "            if self.cont_bad_frames >= self.bad_frames_threshold:\n",
    "                self.recent_fit_left = []\n",
    "                self.recent_fit_right = []\n",
    "                self.best_fit_left = None\n",
    "                self.best_fit_right = None\n",
    "        else:\n",
    "            self.last_detected = True\n",
    "            self.cont_bad_frames = 0\n",
    "            self.recent_fit_left.append(left_fit)\n",
    "            self.recent_fit_right.append(right_fit)\n",
    "            if len(self.recent_fit_left) > self.iterative_win:\n",
    "                self.recent_fit_left = self.recent_fit_left[1:]\n",
    "                self.recent_fit_right = self.recent_fit_right[1:]\n",
    "            if len(self.recent_fit_left) > 0:\n",
    "                self.best_fit_left = np.mean(self.recent_fit_left, axis=0)\n",
    "                self.best_fit_right = np.mean(self.recent_fit_right, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "The whole pipeline for identifying lane and visualize the lane lines, drivable area, lane curvature and vehicle offset is defined in adv_find_lane_lines().\n",
    "\n",
    "When the pipeline is used for finding lane lines in video frames, it keeps tracking the lane lines in previous frames and optimize its lane line searching based on the lane information in previous frames.\n",
    "In case it fails to detect lane lines continuously in certain frames, it switches back to searching with sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_find_lane_lines(img, cam):\n",
    "    '''\n",
    "    Pipeline for finding lane lines in image/video frame.\n",
    "    It identifies the lane lines and visualize the lane line, drivable area,\n",
    "    lane curvature and vehicle offset from lane center.\n",
    "    \n",
    "    img: raw image/frame\n",
    "    cam: Object of camera matrix and distortion coefficients    \n",
    "    '''\n",
    "    # 1. Undistort\n",
    "    undist_img = cv2.undistort(img, cam.mtx, cam.dist, None, cam.mtx)\n",
    "    # 2. Color & Gradient threshold\n",
    "    color_binary, combined_binary = color_gradient_threshold(undist_img)\n",
    "    # 3. Perspective Transform\n",
    "    transformed, M = perspective_transform(combined_binary)\n",
    "    \n",
    "    result_img = img\n",
    "    detected = False\n",
    "    \n",
    "    # 4. Search Lane line pixels and fit/draw 2nd degree polynomial line\n",
    "    if (lane_line.last_detected == False) or (lane_line.best_fit_left is None):\n",
    "        detected, left_fit, right_fit, sliding_win_img = search_sliding_window(transformed)\n",
    "        if detected == True:\n",
    "            fitted, left_fitx, right_fitx, ploty = draw_lane(transformed, left_fit, right_fit)\n",
    "        else:\n",
    "            if lane_line.best_fit_left is not None:\n",
    "                left_fit = lane_line.best_fit_left\n",
    "                right_fit = lane_line.best_fit_right\n",
    "                fitted, left_fitx, right_fitx, ploty = draw_lane(transformed, left_fit, right_fit)\n",
    "            else:\n",
    "                return result_img\n",
    "    else:\n",
    "        detected, left_fit, right_fit = search_around_poly(transformed, lane_line.best_fit_left, lane_line.best_fit_right)\n",
    "        if detected == True:\n",
    "            fitted, left_fitx, right_fitx, ploty = draw_lane(transformed, left_fit, right_fit)\n",
    "        else:\n",
    "            left_fit = lane_line.best_fit_left\n",
    "            right_fit = lane_line.best_fit_right\n",
    "            fitted, left_fitx, right_fitx, ploty = draw_lane(transformed, left_fit, right_fit)\n",
    "            \n",
    "    # 5.1 Calculate curvate and vehicle position relative to lane center\n",
    "    left_curverad, right_curverad = measure_curvature_real(left_fitx, right_fitx, ploty)\n",
    "    \n",
    "    # 5.2 Calculate vehicle position to the lane center\n",
    "    car_offset, lane_width = cal_car_pos(img, left_fit, right_fit)\n",
    "    print(\"Vehicle offset from lane center: %.2fm\" % car_offset)\n",
    "\n",
    "    # 6. Sanity Check\n",
    "    ret = is_valid_lane(left_curverad, right_curverad, car_offset, lane_width)\n",
    "    if (ret == False) or (detected == False):\n",
    "        lane_line.update(None, None)\n",
    "    else:\n",
    "        lane_line.update(left_fit, right_fit)\n",
    "        \n",
    "    curvature = (left_curverad + right_curverad)/2\n",
    "    print(\"Lane Curvature: %.2fm\" % curvature)\n",
    "    \n",
    "    # 7. Convert back to original perspective  \n",
    "    inv_img, Minv = perspective_transform(fitted, inverse=True)\n",
    "\n",
    "    result_img = cv2.addWeighted(img, 1, inv_img, 0.4, 0)\n",
    "    \n",
    "    # 8. Show Curvature and car offset\n",
    "    cv2.putText(result_img, 'Radius of Lane Curvature: {:d} m'.format(int(curvature)), (130, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color=(255,0,0), thickness=3)\n",
    "  \n",
    "    if car_offset < 0:\n",
    "        cv2.putText(result_img, 'Vehicle offset from lane center: {:.2f} m (L)'.format(abs(car_offset)), (130, 170), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color=(255,0,0), thickness=3)\n",
    "    else:\n",
    "        cv2.putText(result_img, 'Vehicle offset from lane center: {:.2f} m (R)'.format(abs(car_offset)), (130, 170), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color=(255,0,0), thickness=3)\n",
    "        \n",
    "    return result_img\n",
    "\n",
    "\n",
    "def process_video(image, cam):\n",
    "    result = adv_find_lane_lines(image, cam)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane Finding Test\n",
    "## Lane Finding in Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Lane Finding in Images...\")\n",
    "test_images = glob.glob('./test_images/test*.jpg')\n",
    "tmp_images = glob.glob('./test_images/straight_lines*.jpg')\n",
    "test_images = test_images + tmp_images\n",
    "\n",
    "rows = len(test_images)\n",
    "\n",
    "dst_dir = './output_images/'\n",
    "for image_file in test_images:\n",
    "    lane_line = LaneLine()\n",
    "    (path_filename, ext) = os.path.splitext(image_file)\n",
    "    filename = os.path.basename(path_filename)\n",
    "    result_image_file = dst_dir + filename + \"_final_result\" + ext\n",
    "    img = mpimg.imread(image_file)\n",
    "    result_img = adv_find_lane_lines(img, cam)\n",
    "    \n",
    "    mpimg.imsave(result_image_file, result_img)\n",
    "    \n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original', fontsize=22)\n",
    "    ax2.imshow(result_img)\n",
    "    ax2.set_title('Lane Lines Detection', fontsize=22)\n",
    "\n",
    "print(\"Lane Finding in Images...Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane Finding in Video\n",
    "### Project Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing normal video...\")\n",
    "lane_line = LaneLine()\n",
    "video_output1 = 'output_images/project_video_output.mp4'\n",
    "clip_video1 = VideoFileClip('project_video.mp4')\n",
    "test_clip1 = clip_video1.fl_image(lambda image: process_video(image, cam))\n",
    "test_clip1.write_videofile(video_output1, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the resultant videos inline, or find the video in filesystem (in \"output_images/\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(video_output1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing challenge video...\")\n",
    "lane_line = LaneLine()\n",
    "video_output2 = 'output_images/challenge_video_output.mp4'\n",
    "clip_video2 = VideoFileClip('challenge_video.mp4')\n",
    "test_clip2 = clip_video2.fl_image(lambda image: process_video(image, cam))\n",
    "test_clip2.write_videofile(video_output2, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the resultant videos inline, or find the video in filesystem (in \"output_images/\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(video_output2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harder Challenge Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing harder challenge video...\")\n",
    "lane_line = LaneLine()\n",
    "video_output3 = 'output_images/harder_challenge_video_output.mp4'\n",
    "clip_video3 = VideoFileClip('harder_challenge_video.mp4')\n",
    "test_clip3 = clip_video3.fl_image(lambda image: process_video(image, cam))\n",
    "test_clip3.write_videofile(video_output3, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the resultant videos inline, or find the video in filesystem (in \"output_images/\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(video_output3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_carnd-term1)",
   "language": "python",
   "name": "conda_carnd-term1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
